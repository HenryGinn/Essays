\section{Particular Issues}
\label{sec:ParticularIssues}

\subsection{Group Design Module}

The following is some feedback I wrote to the student representatives about the running of the group design module by Suwan Jayasinghe. I believe that the department are actually planning on doing something about these issues, although after his involvement with the review process for the module has concluded. I am including it here because I highly doubt that we are the first year group to have had issues with this module, and something should have been done earlier. I also want to highlight the clear lack of a peer review process that allows these problems to occur.

Suwan is the author of chaos and confusion, and I have absolutely nothing positive to say about his handling of the group design module. Without exaggeration he has managed to go wrong at almost every single turn which is honestly impressive given that his one and only job is to tell us about this module. I have no idea how he has been running this module for 10 years and is still so unbelievably incompetent. I would also like to note that his superiority complex is first of all not helpful, and secondly it is completely and utterly unwarranted. The group design module is a train wreck and it is entirely his fault.

First of all there was the requirement that there are an equal number of power systems engineers and mechanical engineers in each group. Anyone can see the immediate problem with this, if there are not the same number of power systems engineers as mechanical engineers then this is mathematically not possible. When this was pointed out to Suwan he was rude, acted as if we had no common sense, that this was somehow our problem, and ultimately he did nothing to resolve the issue. He seemed surprised when it was revealed that this had caused chaos and that some people had to break this rule in order to form groups. Suwan wanted to completely remove himself from any group selection drama\footnote{I should point out that almost all group drama was caused by Suwan in the first place.} which did not help the situation.

The next major blunder was how the group topics were chosen. We were told that they would be given out on a first come, first served basis, although we would not know what the list of projects was in advance. In my opinion this was the biggest mistake in project selection as it meant the optimal strategy would be for one member of the group to pick what they thought were the best projects and submit. This would be significantly faster than any group that decided to discuss the choice amongst themselves and ensure the whole team was in agreement. This was a big decision and we were forced into reading all options and decided as fast as possible.

Another issue was that the topic selection was also done via a form where you could not see which topics had already been selected by others, and therefore it was likely that a group would unknowingly pick options that were already taken. As part of the form we also had to enter all of our student numbers which was an unnecessary source of stress in an extremely time sensitive environment. There should have been a group selection form submitted beforehand where we stated our groups without any time pressure to remove this issue.

One part of the form asked if we had received permission from the supervisor to do this project. Clearly Suwan had not bothered to read the form as at this stage we were not meant to have messaged the supervisors, or even known who the supervisors were. Over a week later many of us were becoming concerned as determining which group had which project should have been a trivial task given that it was determined by who submitted first, yet we had not heard anything. 

Suwan's lack of communication on this matter was a problem. There was a message on the Moodle forum that was never answered by Suwan saying that two groups had received confirmation from the same supervisor. This caused considerable confusion and panic, firstly because we had no idea where they had found this information, and secondly because we were under the impression that it was due to timing, and not supervisor confirmation. I contacted the supervisor for one of my projects who told me that another group had contacted them first so they were the group they had gone with. I heard many stories about people contacting supervisors and hearing similar things, yet Suwan would later claim that all the supervisors had been briefed on how the group selection worked which was clearly false.

Almost two weeks after we had submitted the form I had a meeting with Suwan where he revealed that 25 students were still not even in groups, and that was causing the delay. If this was the situation then it should have been communicated to everyone, yet no emails or Moodle announcements were given, and the question on the Moodle was also ignored. I would like to note that 25 students not being in groups is further evidence of how poorly run this module was. 

Another factor that significantly contributed to the confusion was the out of date Moodle page that gave instructions on how to choose a problem. It said we needed to arrange meetings with the supervisors and get confirmation from them, in contradiction to what we had been told, but in agreement with how the supervisors appeared to understand the arrangement. I later found out that Suwan is not in charge of the Moodle page which I think is a strange choice by the department. Even thought this is the case, as module coordinator he should still be responsible for ensuring that the correct information is on the Moodle page.

Once the projects were allocated another issue arose where some projects were more focused on mechanical engineering but had more power systems students, and other projects had the opposite problem. A solution suggested by a friend of mine was to have individuals sign up to the projects directly. Supervisors would then submit their choices of who they wanted, perhaps based on a small statement provided by the students. While this is a more complicated system, it would significantly help ensure that students were assigned to suitable projects, and that each group had the right mix of skills to give an even workload.

Suwan's attitude is also a large problem. He seems to think he is the only one with common sense, and if you say or ask something that he things is stupid then he will react as if you had three heads. For example, in the second group project briefing there were several questions about formatting of the report. Many students have had experiences of strict formatting requirements in the past, some even losing marks for things such as using the wrong font size. Suwan was apparently completely unaware of this extremely common experience, and instead of realising that this needed significant clarification he continued to act in disbelief that we asked such questions. There was a similar response when asking about how the peer review process works, even though it is different to the standard at many other universities where groups review the work of other groups.

Suwan likes to clear himself of any blame by hosting question and answer sessions where he gives people the opportunity to ask any questions. This would be nice, apart from the fact that he assumes that this means everyone understands everything perfectly after the session. He does not seem to be aware that many people may not want to ask a question that may sound stupid in front of an entire room of people, especially given the anticipated reaction by Suwan. That people may think of more questions later on was also not considered by Suwan, and it was extremely rare to get answers from him on the Moodle or via email.

\subsection{Academic Integrity Policy}

The following is a slightly revised version of a post I put on Unitu about the department's policy on academic integrity and collusion. I am including it here because I have received no response from the department. In my opinion, the current academic misconduct policy is no more than a box ticking exercise by the university, and a weak attempt at trying to prevent the worst of academic misconduct. It is confused about what behaviour is important to stop and is also almost completely unenforceable.

As an example of where the policy makes little sense, consider what it says about use of AI. When we are allowed to use ChatGPT we are meant to say what prompts we used, what output it gave us, and how we used it. If everyone were to actually follow this, that section of the appendix would be at least half the size of the whole document. Does anyone actually do this? There are coursework questions that are so well suited to ChatGPT that the lecturer may as well tell us directly ``I want to read more ChatGPT drivel, can you help?".

When the lecturers see that barely anyone at all has a section explaining the use of AI in their work, do they assume that no ChatGPT was used by any of the students? The students do not care and the markers do not care, but ultimately why should they? Suppose I use ChatGPT to explain the difference between ``practise” and ``practice”, am I meant to put a footnote after I use either of those words telling the marker to see the appendix for details on how I learnt this from ChatGPT?

UCL also has a very strict collusion policy, although it is completely unenforceable. From my understanding, doing much beyond a very surface level proof-read of someone else’s work counts as collusion, and this seems absurd to me. My takeaway from the academic integrity lectures was that UCL would prefer it if we entered into some sort of pod at the start of the year and emerged at the end, doing all work in complete solitude. 

I think UCL is confused on what is important and what is not when it comes to collusion. Helping a friend fix a bug in their code, seeing if your numbers are in the same ball-park, getting opinions on a figure they have made, or even explaining a concept are all examples of behaviour that is completely acceptable in my eyes. These do not change the difficulty of the task, everyone still has ownership over their work, and they still learn the things they were meant to learn from the task. This is what the collusion policy should be about, and actions that challenge this such as copying and editing sections of work, or allowing someone to appear as if they understand something when they don’t are what should be the focus.

As an example of an attempt to stop collusion, consider the FEA coursework where one of the geometry parameters depended on our surname. Firstly, this was a single parameter, you could easily check results with friends by changing a single character in your code. Secondly, a quarter of the cohort would have the same parameter value as you anyway meaning you could easily find someone with the exact same task as you anyway. The tasks were far too similar meaning it could not be effective as a way of stopping collusion.

I’d also like to note that for one value of this parameter, all elements would be rectangular. This meant that the maps from the reference element to the practical elements would be significantly simpler than the map to a general quadrilateral, and resulted in a constant diagonal jacobian that was the same for all elements. Even though I was in this group and did it in full generality anyway, I will moan about this. Measures to stop collusion should not make the task easier for some students over others. Overall this measure was half-hearted, ill thought out, and had extremely negligible effects on preventing collusion. Other modules do not have any measures at all.

There is also very little done to stop poor academic practices. If I were to spot an extra factor that changed a number by 5\% early on in a series of calculations, I would have to update all the equations that were influenced by that. Alternatively, it would be much easier to just change one number in one equation, and not bother updating anything else. No one is actually going to bother checking that. I’d never do this because I would not be able to sleep at night, but you could, and students almost certainly have done this.

For another example, consider the FEA coursework again where we had to put our code in the appendix. If it looks legitimate then the markers are not going to check it, and there was at least one student who submitted code that did not even run and they were not caught. They should have asked us to submit our MATLAB files directly. References can also be extremely sloppy as realistically they are not going to be checked. Referencing badly is significantly faster than referencing properly. Many students will go to Google Scholar, pick any of the top three results of their search, and no one will ever know.

I’d like to speak more broadly about UCL’s attitude to aids and resources available and how it relates to academic misconduct. For example, at UCL we are not even allowed the more advanced calculators in exams. A calculator capable of single variable integration would only be cheating if we were being tested on our ability to integrate simple functions, and on a masters level course why would we be tested on that, we are not back at A-level. This is an example of where UCL seems confused on what is appropriate to test us on.

My undergraduate course provides a good comparison. For online exams there was no calculator restriction and we were allowed to use Wolfram Alpha, but it was only useful for checking answers or general grunt work that was irrelevant to the difficulty of the task. We were even allowed complete access to the internet for online exams, although it was not very helpful. This was because the questions were not standard and could not be easily found on the internet. It was understood that these were assistive tools, and the difficulty, problem solving, and effort had to come from you. This is not understood at UCL.

This attitude worked at my undergraduate university because the difficulty, problem solving, and effort could only come from you, it was not feasible to get it from somewhere else. This brings me to my final point about the core of why UCL’s academic misconduct policy is bad – the tasks we are given lend themselves to academic misconduct. If we are given a question where it is possible to generate a bunch of waffle from ChatGPT and then remove obvious ChatGPTisms like, ``this underscores xyz” or “enhancing abc and promoting xyz”, then many the students are going to do just that. The issue here is that we should not be given questions like this in the first place, it should be the case that if you do not understand then it will be obvious, no matter how much you poke and prod an AI.

The unenforceability of many aspects of the policy is also a huge problem. There needs to be an understanding that a deterrent depends on the immediacy, magnitude, and likelihood of consequences. The department cannot expect even the majority of students to comply when the chance of getting caught is zero for all practical purposes. For example, collusion will not be detected unless they find sections which are nearly identical between two submissions. As we are not completely stupid, any students who are colluding would not do this. I would not be surprised that the only real check that is done is Turnitin, and as we have access to this ourselves this will only catch the most foolish of plagiarisers. This policy reminds me of the BBC’s TV detector vans and other fruitless efforts to enforce TV licensing.

UCL should reevaluate their priorities and also what they are capable of enforcing. Personally, I think they should stop caring about regular behaviour between friends working together, and focus more on the more important issue of students acting in bad faith and getting higher marks than they would otherwise. In particular I think the issue is with coursework exercises with a very narrow solution - if you ask everyone to do the same calculation, they are going to compare answers.

I would prefer to see coursework exercises that are more open and allow students to take them in different directions. In this case engineering intuition and understanding of concepts would be tested more heavily, and students would not be able to collude as easily on these more important parts of the assessment without ending up with extremely similar submissions. Failing this, could UCL please drop the facade that they care about trivial details such as telling a friend what Excel function to use when they ask for help? Engineering and university work should be more collaborative than what the policy suggests. Please note that this is not a request for more group design work.